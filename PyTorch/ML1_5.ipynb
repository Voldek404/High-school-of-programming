{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "ls8BPtFn0Uop"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcgW51u1yP8C"
   },
   "source": [
    "# **Занятие 5. Классификация изображений**\n",
    "\n",
    "https://vk.com/lambda_brain\n",
    "\n",
    "Классификация изображений -- это уже полноценная задача реального мира. Самое приятное здесь в том, что нету качественного алгоритмического отличия классификации изображений от классификации других наборов данных. Нейронные сети работают таким магическим образом, что мы просто подаём им на вход наборы \"байтов\", и не важно, что это -- абстрактные значения функций или пикселы изображений, они все анализируются нейросетью по единой схеме.\n",
    "\n",
    "Мы познакомимся с основными принципами классификации изображений в PyTorch, а также изучим смежные вопросы, связанные с загрузкой объёмных датасетов с изображениями в программу и их удобной визуализацией.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ls8BPtFn0Uop"
   },
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-ssFltrylmg"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "В качестве первого примера воспользуемся распространённым датасетом изображений **MNIST**, на котором создано наверное максимальное количество обучающих уроков по нейронным сетям.\n",
    "\n",
    "MNIST подготовлен **Яном ЛеКуном** -- ведущим мировым специалистом по нейронным сетям, и представляет собой датасет для обучения распознаванию рукописных цифр. Обучающая выборка содержит 60,000 изображений, и тестовая -- 10,000. Каждое изображение чёрно-белое размером 28x28 пикселов.\n",
    "\n",
    "Импортируем нужные модули:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8abpHoRq6V-L",
    "ExecuteTime": {
     "end_time": "2025-11-22T11:20:46.833893Z",
     "start_time": "2025-11-22T11:20:46.141836Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGDiRyHTy4kF"
   },
   "source": [
    "Задаём гипер-параметры:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UfwVbruB8gQb",
    "ExecuteTime": {
     "end_time": "2025-11-22T11:22:55.044869Z",
     "start_time": "2025-11-22T11:22:55.042069Z"
    }
   },
   "source": [
    "\n",
    "input_size = 28*28     # Размер изображения в точках\n",
    "hidden_size = 500      # Количество нейронов в скрытом слое\n",
    "num_classes = 10       # Количество распознающихся классов (10 цифр)\n",
    "n_epochs = 5          # Количество эпох\n",
    "batch_size = 4         # Размер мини-пакета входных данных\n",
    "lr = 0.01              # Скорость обучения"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvCFT8OvzLik"
   },
   "source": [
    "Большое количество готовых датасетов и различные удобные методы их загрузки и обработки собраны в модуле torchvision. Имеется в нём конечно и MNIST.\n",
    "\n",
    "В конструкторе датасета MNIST указывается, как правило, каталог root, где будет локально размещён соответствующий датасет, download -- необходимость скачивания датасета для его локального использования, train -- признак, является ли данный датасет обучающим или тестовым, и трансформация transform, которую надо выполнить над данным датасетом. Последняя фича весьма полезна, потому что исходные изображения нередко требуется предварительно обрабатывать под конкретный формат их дальнейшего анализа. В нашем случае просто укажем стандартную трансформацию преобразования входных данных в тензоры.\n",
    "\n",
    "Убедимся, что размеры датасета совпадают с официально объявленными.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0eX2jdtCLzWd",
    "ExecuteTime": {
     "end_time": "2025-11-22T11:20:57.116888Z",
     "start_time": "2025-11-22T11:20:46.856244Z"
    }
   },
   "source": [
    "import torchvision.transforms as transforms\n",
    "mnist_trainset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_testset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "print(len(mnist_trainset))\n",
    "print(len(mnist_testset))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3Wg4nsIzpkF"
   },
   "source": [
    "Так как наш датасет большой по размеру, обрабатывать его надо небольшими порциями, мини-пакетами, как рассказывалось во втором занятии -- с помощью загрузчиков данных DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "brvDSwBVNqqS",
    "ExecuteTime": {
     "end_time": "2025-11-22T11:22:52.829461Z",
     "start_time": "2025-11-22T11:22:52.826506Z"
    }
   },
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_trainset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True) # загрузчик обучающих данных\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_testset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False) # загрузчик тестовых данных"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHdERlgdz2dG"
   },
   "source": [
    "Добавим наш стандартный шаг обучения."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TCBltxfGOChj",
    "ExecuteTime": {
     "end_time": "2025-11-22T12:06:22.376116Z",
     "start_time": "2025-11-22T12:06:22.373059Z"
    }
   },
   "source": [
    "# импортируем нужные библиотеки\n",
    "import torch\n",
    "import numpy as np # всегда пригодится :)\n",
    "from torch.nn import Linear, Sigmoid\n",
    "\n",
    "# инициализируем девайс\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# добавляем типовую функцию \"шаг обучения\"\n",
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    def train_step(x, y):\n",
    "        model.train()\n",
    "        yhat = model(x)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return loss.item()\n",
    "    return train_step"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmngiGejz5lW"
   },
   "source": [
    "Структура нашей модели будет представлять собой двуслойную нейронную сеть -- она ничем не отличается от модели, применявшейся в прошлом занятии при анализе абстрактного датасета (линейный вход + ReLU + линейный выход).\n",
    "\n",
    "Воспользуемся лоссом CrossEntropyLoss() и методом оптимизации Adam.\n",
    "\n",
    "Обратите внимание, что мы задаём уже не тысячи, а **всего две эпохи** -- процесс обучения на таком довольно объёмном датасете потребует уже прилично времени (несколько минут на одну эпоху).\n",
    "\n",
    "Единственное дополнение -- мы применяем метод reshape(-1, 28 * 28), чтобы изменить входной трёхмерный формат изображения 1x28x28 (глубина цветности x размеры) на нужный нам одномерный вектор длиной 784 значения. Параметр -1 означает, что преобразование выполняется в одномерный результат.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZlW-CkjeOn2u",
    "ExecuteTime": {
     "end_time": "2025-11-22T12:21:11.948291Z",
     "start_time": "2025-11-22T12:20:16.507838Z"
    }
   },
   "source": [
    "from torch import optim, nn\n",
    "\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, num_classes))\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        loss = train_step(images, labels)\n",
    "    print(epoch)\n",
    "\n",
    "print(model.state_dict())\n",
    "print(loss)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "OrderedDict([('0.weight', tensor([[-0.0065, -0.0295, -0.0172,  ..., -0.0077, -0.0317,  0.0272],\n",
      "        [-0.0306,  0.0149,  0.0325,  ...,  0.0018,  0.0324, -0.0316],\n",
      "        [-0.0016, -0.0256,  0.0182,  ..., -0.0073, -0.0060,  0.0125],\n",
      "        ...,\n",
      "        [ 0.0243, -0.0074, -0.0027,  ..., -0.0318,  0.0170, -0.0274],\n",
      "        [ 0.0165,  0.0220,  0.0113,  ..., -0.0287, -0.0342, -0.0102],\n",
      "        [ 0.0004, -0.0285,  0.0235,  ..., -0.0226,  0.0311,  0.0276]])), ('0.bias', tensor([-3.6393e-02, -1.1639e-01, -5.0114e-02, -4.8848e-01, -7.1532e-02,\n",
      "        -3.4722e-01, -1.8594e-02, -4.5827e-02, -9.0146e-02,  8.7780e-02,\n",
      "        -4.9195e-01, -4.1775e-01, -2.6505e-01, -3.7672e-01, -6.5317e-02,\n",
      "        -2.6545e+00, -3.5798e-01, -8.3253e-01, -1.9934e+00, -2.3268e-01,\n",
      "        -7.2201e-02, -6.4401e-01, -6.4924e-02, -1.0624e-01, -4.2828e-02,\n",
      "        -7.8051e-01, -3.8578e-01, -2.9336e-02, -1.1323e-01, -3.6897e-01,\n",
      "        -8.8965e-02, -5.8279e-01, -6.2851e-02, -8.9032e-02, -4.6236e-01,\n",
      "        -3.7466e-01, -4.1108e-02, -1.3296e-01, -8.0616e-02, -2.1619e-01,\n",
      "        -5.9635e-02, -6.8233e-01, -3.2432e+00, -5.2279e-02, -4.3179e-02,\n",
      "        -5.0848e-02, -6.0517e-02, -3.8387e-02, -6.3650e-01, -2.0300e-02,\n",
      "        -1.0152e-01, -2.7417e-02, -7.2755e-02, -3.2028e-02, -8.8510e-02,\n",
      "        -1.0334e-01, -5.0108e-01, -5.3010e-02, -6.9200e-02, -5.0321e-01,\n",
      "        -1.3966e-01, -7.9591e-01, -7.1502e-02, -9.3799e-02, -2.6342e-01,\n",
      "        -4.4512e-01, -8.8911e-02, -1.1338e+00, -1.8404e-01, -3.8046e-02,\n",
      "        -8.5974e-02, -6.4054e-02, -1.6195e-01, -6.8398e-02, -3.4399e-02,\n",
      "        -9.6544e-02, -4.3652e-02, -8.1753e-01, -4.8379e-02, -1.0074e-01,\n",
      "        -1.1398e-01, -3.0646e-02, -1.0314e-01, -7.4349e-02, -7.0409e-02,\n",
      "        -6.7308e-02, -5.5426e-02, -6.5576e-02, -6.2353e-01, -5.6425e-02,\n",
      "        -3.1416e-01, -3.3369e-01, -3.6481e-01, -5.4903e-02, -4.0915e-01,\n",
      "        -3.2379e-02, -7.2931e-02, -4.0995e-02, -8.2335e-01, -3.6146e-02,\n",
      "        -8.0081e-02, -5.5372e-02, -3.9409e-02, -6.8667e-02, -9.3262e-02,\n",
      "        -1.7114e+00, -6.3982e-02, -5.5293e-01, -3.1401e-01, -6.6852e-02,\n",
      "        -6.3367e-02, -8.7201e-02, -1.4131e-01, -1.1486e-01, -4.5908e-01,\n",
      "        -6.9523e-02, -5.1320e-01, -5.5089e-02, -4.3755e-01, -5.2525e-01,\n",
      "        -7.1259e-01, -1.7536e+00, -6.9877e-02, -5.8625e-02, -8.7954e-02,\n",
      "        -5.3772e-02, -1.7216e-01, -2.8311e-02, -7.7457e-02, -3.7838e-01,\n",
      "        -8.2437e-02, -7.7483e-02, -5.7549e-01, -3.4166e-01, -4.9710e-02,\n",
      "        -9.4904e-02, -7.1715e-02,  3.9315e-01, -1.6511e+00, -4.0682e-02,\n",
      "        -3.9402e+00, -2.6534e+00, -8.0215e-02, -1.8565e-01, -7.5362e-02,\n",
      "        -5.4721e-02, -5.7496e-01, -6.0850e-02, -3.8548e-02, -4.0690e-02,\n",
      "        -4.3053e-01, -4.3827e-02, -6.8295e-02, -7.9678e-02, -6.0239e-01,\n",
      "        -6.8681e-02, -4.0121e-01, -2.5310e-01, -7.1259e-02, -1.0230e-01,\n",
      "        -4.3494e-02, -1.1634e-01, -3.1130e-01, -2.4860e-02, -2.5157e-02,\n",
      "        -3.7724e-02, -4.3626e-01, -9.3884e-01, -7.7315e-01, -2.3815e-01,\n",
      "        -3.2937e-01, -8.3299e-02, -5.2887e-01, -1.5501e-01, -1.5142e-01,\n",
      "        -5.0969e-01, -2.2996e+00, -4.8843e-02, -4.3855e-01, -8.1202e-02,\n",
      "        -6.6976e-02, -3.7724e-02, -8.5159e-01, -6.2906e-02, -2.9722e-02,\n",
      "        -5.2786e-01, -6.9193e-02, -1.5404e-02, -4.8571e-01, -3.7339e-01,\n",
      "        -8.7312e-02, -5.2216e-02, -6.8784e-02, -7.9060e-02, -5.1941e-02,\n",
      "        -1.1775e-01, -6.5833e-02, -4.2557e-02, -2.5242e-02, -3.5060e-01,\n",
      "        -6.2575e-02, -7.1379e-01, -7.2907e-02, -7.5458e-02, -5.1426e-02,\n",
      "        -6.2545e-02, -6.5502e-02, -2.6695e-01, -4.5889e-01, -5.8479e-02,\n",
      "        -1.0118e-01, -8.5857e-02, -8.9959e-01, -1.1455e-01, -7.8513e-02,\n",
      "        -4.1928e-02, -4.0629e-02, -9.2723e-02, -2.4814e-02, -3.7747e-02,\n",
      "        -5.5816e-02, -6.5033e-02, -1.0379e-01, -3.0486e-02, -3.2299e-02,\n",
      "        -1.1858e-01, -9.0818e-02, -4.7155e-02, -2.7357e+00, -8.6965e-01,\n",
      "        -2.6725e-01, -6.0818e-02, -2.7808e-02, -1.1152e-02, -1.5734e-01,\n",
      "        -7.5407e-02, -3.7015e-01, -8.0117e-02, -6.4195e-02, -3.2516e-02,\n",
      "        -8.1766e-02, -5.0132e-01, -2.4534e-02, -4.0215e-01, -2.9033e-01,\n",
      "        -5.1753e-01, -6.5419e-02, -4.3866e-01, -2.2433e-01, -3.6726e-02,\n",
      "        -2.4527e-02, -2.1756e-01, -2.9572e-02, -3.5412e-02, -1.1411e-01,\n",
      "        -1.0549e-01, -2.7021e-02, -7.2094e-02, -1.1607e+00, -1.7309e+00,\n",
      "        -1.8703e+00, -5.9492e-02, -2.6471e-01, -4.5048e-02, -1.2096e+00,\n",
      "        -6.5885e-02, -3.0153e-01, -4.8053e-02, -3.6943e-02, -4.1587e-02,\n",
      "        -8.2717e-02, -7.4380e-02, -7.1215e-01, -4.2601e-01, -6.4233e-02,\n",
      "        -4.6712e-02, -6.3130e-02, -2.1388e-01, -1.1276e-01, -1.2622e-02,\n",
      "        -7.5310e-02, -6.9037e-02, -7.5739e-02, -1.1595e-02, -3.3737e-02,\n",
      "        -3.8202e-01, -5.6304e-02, -4.7717e-02, -6.4524e-02, -8.7219e-02,\n",
      "        -4.2908e-01, -7.9668e-02, -2.7203e-01, -1.1140e+00, -8.0529e-02,\n",
      "        -5.1386e-02, -5.8849e-02, -6.8261e-01, -7.6393e-02, -7.3968e-02,\n",
      "        -4.3515e-02, -8.1373e-02, -2.8274e-02, -1.0295e-01, -7.1310e-01,\n",
      "        -8.0248e-02, -3.9689e-01, -1.5633e-01, -5.6902e-01, -9.5083e-02,\n",
      "        -2.1814e+00, -3.4797e-02, -9.2826e-02, -2.0653e+00, -2.7015e-02,\n",
      "        -2.5386e-01, -8.4942e-02, -9.5901e-02, -1.2193e+00, -2.8477e-02,\n",
      "        -3.6232e-01, -8.5779e-02, -3.0778e-02, -1.0064e-01, -7.4549e-02,\n",
      "        -4.6122e-01, -6.2577e-02, -6.1586e-01, -1.9710e-01, -1.0108e+00,\n",
      "        -2.1109e-01, -7.4063e-02, -6.1765e-02, -2.1861e-01, -7.4811e-02,\n",
      "        -5.4654e-02, -8.7068e-02, -2.0277e+00, -9.0539e-02, -1.0241e-01,\n",
      "        -8.6738e-02, -6.3853e-02, -7.0153e-02, -4.2822e-02, -1.1779e-01,\n",
      "        -1.7713e+00, -6.7671e-02, -1.7591e-01, -9.5484e-02, -3.4434e-02,\n",
      "        -8.0561e-01, -4.3623e-02, -9.3367e-02, -9.3457e-02, -3.5966e-03,\n",
      "        -7.4817e-02, -3.6986e-01, -1.3508e-01, -1.0646e-01, -1.8016e-01,\n",
      "        -2.1502e-01, -9.0238e-01, -8.0995e-02, -1.6408e+00, -5.3979e-02,\n",
      "        -4.6359e-02, -1.4391e+00, -8.4816e-02, -4.3021e-02, -6.7667e-02,\n",
      "        -7.4954e-02, -1.4128e+00, -6.7888e-01, -1.4735e+00, -2.4641e-02,\n",
      "        -6.7031e-02, -5.9994e-02, -1.1753e-01, -6.0889e-02, -1.0002e+00,\n",
      "        -9.1842e-02, -3.3044e-02, -2.1820e+00, -1.2205e-01, -6.7740e-02,\n",
      "        -6.8028e-02, -3.6281e-01, -3.2082e-01, -1.7167e+00, -1.6070e-01,\n",
      "        -2.9062e-01, -1.8991e-01, -1.1609e-01, -8.0945e-01, -7.0344e-02,\n",
      "        -4.2951e-02, -7.9768e-01, -1.3823e+00, -4.2018e-02, -2.8798e-01,\n",
      "        -7.7745e-02, -5.2309e-02, -3.1247e-01, -7.8921e-02, -8.7367e-02,\n",
      "        -3.0110e-01, -4.7932e-02, -5.0826e-02, -3.9053e-02, -2.7605e-02,\n",
      "        -6.1858e-02, -3.8660e-02, -2.7743e-02, -1.6983e+00, -8.2570e-02,\n",
      "        -7.3290e-02, -3.3100e-01, -6.8919e-02, -3.0449e-02, -8.0670e-02,\n",
      "        -3.9959e-01,  1.8733e-01, -8.2926e-02, -4.3051e-02, -2.7755e-02,\n",
      "        -4.8300e-02, -2.4813e-02, -3.2230e-02, -9.5162e-01, -6.6883e-02,\n",
      "        -5.1318e-02, -3.3615e+00, -3.0850e-01, -5.4366e-01, -1.2094e+00,\n",
      "        -4.2169e-01, -5.3117e-02, -6.6017e-02, -8.1527e-03, -1.4032e-01,\n",
      "        -7.2655e-02, -6.4771e-02, -7.7999e-02, -6.9005e-02, -7.4854e-01,\n",
      "        -5.6113e-02, -4.0426e-02, -3.2088e-02, -7.4629e-02, -4.4887e-01,\n",
      "        -1.6597e-02, -1.4470e-01, -7.3587e-02, -3.8746e-01, -9.3924e-02,\n",
      "        -5.1486e-02, -7.0288e-01, -9.2629e-01, -8.0323e-02,  3.7377e-01,\n",
      "        -2.8405e-01, -8.4782e-02, -4.3418e-02, -2.0755e-02, -3.8916e-02,\n",
      "        -7.7830e-02, -4.6723e-02, -6.6253e-01, -6.5226e-01, -5.7855e-02,\n",
      "        -2.4396e-02, -4.6189e-02, -8.7311e-02, -6.0251e-02, -3.8016e-02,\n",
      "        -5.3961e-01, -1.0849e-01, -7.4005e-02, -4.0784e-02, -8.4857e-02,\n",
      "        -8.4915e-02, -3.0739e+00, -3.0507e-01, -7.2977e-02, -5.8609e-01,\n",
      "        -7.3991e-02, -1.4048e-02, -1.2300e+00, -3.0396e-01, -3.2584e-02,\n",
      "        -3.1217e-01, -2.7703e-01, -1.2424e-01, -3.8091e-01, -6.3455e-02,\n",
      "        -6.0810e-02, -1.3575e+00, -3.9034e-02, -5.3801e-02, -5.9341e-02])), ('2.weight', tensor([[-0.0657, -0.0204, -0.0241,  ..., -0.1039, -0.0554,  0.0269],\n",
      "        [ 0.0143, -0.0925,  0.0780,  ..., -0.0291, -0.0373, -0.0929],\n",
      "        [-0.0540, -0.0678, -0.0534,  ..., -0.0557,  0.0635,  0.0265],\n",
      "        ...,\n",
      "        [ 0.0206, -0.1025,  0.0237,  ..., -0.0853, -0.0279, -0.0506],\n",
      "        [-0.0447, -0.0308, -0.0890,  ..., -0.0881, -0.0249,  0.0137],\n",
      "        [ 0.0393, -0.0678, -0.0707,  ..., -0.1015, -0.0731, -0.1456]])), ('2.bias', tensor([ 0.3781, -1.3695,  0.0066,  0.0126, -0.3694, -0.8220, -0.0793, -1.4656,\n",
      "         1.9172,  0.2620]))])\n",
      "0.0064750658348202705\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpV9EAsF0mK8"
   },
   "source": [
    "Теперь проверим, с какой точностью наша модель проверит 10,000 тестовых изображений:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "L-zC19n5X_-7",
    "ExecuteTime": {
     "end_time": "2025-11-22T11:25:50.846095Z",
     "start_time": "2025-11-22T11:25:49.429650Z"
    }
   },
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Точность: {} %'.format(100 * correct / total))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность: 93.28 %\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beRRn3tq00-s"
   },
   "source": [
    "Около 90% -- это очень хороший результат для тренировки всего в течение пяти минут за две эпохи. Обратите внимание, что обученная модель время на распознавание даже такого приличного по объёму датасета уже практически не тратит.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Вычисление, конечно, может занимать не минуты, а часы, сутки, недели и месяцы работы. Чтобы продолжить работу с нашей обученной моделью уже в прикладных задачах, нам надо прежде всего её сохранить в файле (в PyTorch принято, что расширение файла с полностью сохранённой моделью -- .pt).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4NXf2S6EY8PR",
    "ExecuteTime": {
     "end_time": "2025-11-22T11:21:53.620378Z",
     "start_time": "2025-11-22T11:21:53.610964Z"
    }
   },
   "source": [
    "torch.save(model, 'mnist_full.pt')"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MgEmnMa1CzC"
   },
   "source": [
    "Сохранение выполняется в виртуальной машине гугла, где мы загрузили датасет MNIST. Загрузка модели выполняется так же просто:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yjjzYGrHjHlW",
    "ExecuteTime": {
     "end_time": "2025-11-22T11:21:53.730009Z",
     "start_time": "2025-11-22T11:21:53.631322Z"
    }
   },
   "source": [
    "model = torch.load(\"mnist_full.pt\")\n",
    "model.eval()"
   ],
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001B[1mdo those steps only if you trust the source of the checkpoint\u001B[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL torch.nn.modules.container.Sequential was not an allowed global by default. Please use `torch.serialization.add_safe_globals([torch.nn.modules.container.Sequential])` or the `torch.serialization.safe_globals([torch.nn.modules.container.Sequential])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mUnpicklingError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmnist_full.pt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/serialization.py:1529\u001B[0m, in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[1;32m   1521\u001B[0m                 \u001B[38;5;28;01mreturn\u001B[39;00m _load(\n\u001B[1;32m   1522\u001B[0m                     opened_zipfile,\n\u001B[1;32m   1523\u001B[0m                     map_location,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1526\u001B[0m                     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpickle_load_args,\n\u001B[1;32m   1527\u001B[0m                 )\n\u001B[1;32m   1528\u001B[0m             \u001B[38;5;28;01mexcept\u001B[39;00m pickle\u001B[38;5;241m.\u001B[39mUnpicklingError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m-> 1529\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m pickle\u001B[38;5;241m.\u001B[39mUnpicklingError(_get_wo_message(\u001B[38;5;28mstr\u001B[39m(e))) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1530\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _load(\n\u001B[1;32m   1531\u001B[0m             opened_zipfile,\n\u001B[1;32m   1532\u001B[0m             map_location,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1535\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpickle_load_args,\n\u001B[1;32m   1536\u001B[0m         )\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mmap:\n",
      "\u001B[0;31mUnpicklingError\u001B[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001B[1mdo those steps only if you trust the source of the checkpoint\u001B[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL torch.nn.modules.container.Sequential was not an allowed global by default. Please use `torch.serialization.add_safe_globals([torch.nn.modules.container.Sequential])` or the `torch.serialization.safe_globals([torch.nn.modules.container.Sequential])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7uotPWL1KwL"
   },
   "source": [
    "После загрузки модели надо обязательно выполнить её метод **eval()** -- это означает, что мы будем пользоваться моделью в режиме её эксплуатации, а не в режимах обучения или каких-то других."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BVbpWvHQ1F2"
   },
   "source": [
    "##**Задание**\n",
    "\n",
    "Попробуйте повысить точность нашей модели:\n",
    "\n",
    "* увеличьте количество эпох;\n",
    "* увеличьте количество нейронов;\n",
    "* добавьте новые скрытые слои;\n",
    "* измените размер мини-пакета.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "* При желании придумайте, как визуализировать работу модели на тестовых данных (показывать изображения и соответствующий им распознанный класс).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9JkYrdVE3bb"
   },
   "source": [
    "Далее мы рассмотрим работу с более сложным датасетом изображений (котики и пёсики), которому потребуется предварительная обработка."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Было бы интересно создать инфраструктуру для проверки различных гиперпараметров\n"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-22T14:50:19.273541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "import torchvision\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "class ResearchInfra:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.input_size = 28 * 28\n",
    "        self.num_classes = 10\n",
    "        self.n_epochs = 9\n",
    "\n",
    "        # данные\n",
    "        transform = transforms.ToTensor()\n",
    "        self.mnist_train = dsets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "        self.mnist_test = dsets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    def get_configs(self):\n",
    "        configs = [\n",
    "            {\"hidden_size\": 500, \"batch_size\": 14, \"lr\": 0.01},\n",
    "            {\"hidden_size\": 550, \"batch_size\": 14, \"lr\": 0.01},\n",
    "            {\"hidden_size\": 600, \"batch_size\": 14, \"lr\": 0.01},\n",
    "            {\"hidden_size\": 650, \"batch_size\": 14, \"lr\": 0.01},\n",
    "            {\"hidden_size\": 700, \"batch_size\": 14, \"lr\": 0.01},\n",
    "            {\"hidden_size\": 750, \"batch_size\": 14, \"lr\": 0.01}\n",
    "        ]\n",
    "        return configs\n",
    "\n",
    "    def make_loaders(self, batch_size):\n",
    "        train_loader = torch.utils.data.DataLoader(dataset=self.mnist_train,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=self.mnist_test,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False)\n",
    "\n",
    "        return train_loader, test_loader\n",
    "\n",
    "    @staticmethod\n",
    "    def make_train_step(model, loss_fn, optimizer):\n",
    "        def train_step(x, y):\n",
    "            model.train()\n",
    "            yhat = model(x)\n",
    "            loss = loss_fn(yhat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            return loss.item()\n",
    "        return train_step\n",
    "\n",
    "    def run_experiment(self, config):\n",
    "\n",
    "        hidden_size = config[\"hidden_size\"]\n",
    "        batch_size = config[\"batch_size\"]\n",
    "        lr = config[\"lr\"]\n",
    "\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        ).to(device)\n",
    "\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        train_loader, _ = self.make_loaders(batch_size)\n",
    "        train_step = self.make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "        loss_history = []\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            for images, labels in train_loader:\n",
    "                images = images.reshape(-1, 28*28).to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                loss = train_step(images, labels)\n",
    "            # print(loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.reshape(-1, 28*28).to(device)\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                loss_history.append(100 * correct / total)\n",
    "\n",
    "        print(f\"Config {config} | Epoch {epoch} | Точность: {100 * correct / total:.2f} %\")\n",
    "\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "\n",
    "research_infra = ResearchInfra()\n",
    "configs = research_infra.get_configs()\n",
    "\n",
    "results = []\n",
    "\n",
    "for cfg in configs:\n",
    "    history = research_infra.run_experiment(cfg)\n",
    "    results.append({\n",
    "        \"config\": cfg,\n",
    "        \"loss_history\": history\n",
    "    })\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "for result in results:\n",
    "    plt.plot(result['loss_history'], marker='o', linewidth=1,  # тонкая линия\n",
    "             label=f\"h={result['config']['hidden_size']}, \"\n",
    "                   f\"bs={result['config']['batch_size']}, \"\n",
    "                   f\"lr={result['config']['lr']}\")\n",
    "\n",
    "plt.title(\"Точность на валидации (%)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config {'hidden_size': 500, 'batch_size': 14, 'lr': 0.01} | Epoch 8 | Точность: 95.94 %\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
